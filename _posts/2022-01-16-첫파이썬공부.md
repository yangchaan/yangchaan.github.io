---
categories: Python
tag: [crawling]
---

# 파이썬과 웹 크롤링

파이썬은 학부 때, 잠깐 사용해본 이후로 써본적이 없었다.

파이썬의 문법과 자료구조에 익숙해질 겸 웹 크롤링에 대해 공부했고, 나도코딩 채널의 파이썬 코딩 무료강의 활용편3 웹 크롤링 영상을 참조했다.

회사에서 gitlab을 사용하는데, gitlab 버전이 낮아 그룹에 있는 프로젝트들을 일일이 clone 해줘야 했다. 

배운 내용을 통해 사내 gitlab에 접속하여 선택한 그룹의 모든 프로젝트의 clone Link를 가져와 모두 clone하는 프로그램을 만들어보았다.

라이브러리, 모듈은 크게 beautifulsoup4와 selenium을 사용했고, 강의 내용에는 requests 모듈에 관한 내용도 있었다.

회사에서 사용하기위해 만든 프로그램이라 올리지는 못하지만, 만들면서 알아간 내용과 나중에 파이썬을 이용한 프로젝트를 진행할 때 다시 복습하기 위해 정리한다.


## HTML
나는 웹 사이트에 대한 지식이 전혀 없는 상태에서 시작했다.

크롤링으로 원하는 정보를 가져오기 위해서는 대충이라도 HTML을 볼 줄 알아야했고, 간단한 크롤링을 위해서는 아래정도의 내용만 알아도 무리가 없었다. 

* 웹 사이트는 HTML이라는 형식으로 쓰여져있다.

* HTML은 HyperText Markup Language의 약자로, 각각의 HTML 태그는 웹 페이지의 디자인이나 기능을 결정하는데 사용된다.

* HTML은 보통 시작태그(여는 태그)와 종료태그(닫는 태그)의 한 쌍으로 구성된다.( <tag></tag> )

* 태그 안에 속성(attribute)를 통해 추가적인 정볼르 제공하여 명령어를 구체화 시킨다. ('속성명'='속성값' 형태를 가짐)

* 시작태그와 종료태그, 그 안의 속성등을 통해 이루어진 명령어를 요소(Element)라고 한다.

* "<div class="tile-title"></div>" 를 예시로 하면, div라는 태그에 class라는 속성과 value로 이루어진 element이다.


## requests
requests는 파이썬에서 http를 사용하기 위해 쓰여지는 라이브러리이다.

웹 크롤링에서 사용하는 이유는 해당 모듈을 이용해 URL에 HTML을 얻을 수 있기 때문이다.

대표적인 사용법은 아래와 같다.

```python
import requests

res = requests.get("http://google.com")
# 어떤 방식의 HTTP요청을 하느냐에 따라 get, post, put, delete 방식에 맞는 함수를 사용.

print("응답코드 :", res.status_code) 
# 200이면 정상
# 응답코드가 200이 아니면 정상적으로 HTML을 가져올 수 없기 때문에 다른방식을 찾아야 함

res.raise_for_status()  # 응답코드가 정상이 아니면 바로 프로그램을 종료시켜줌

print(res.text) # HTML 출력
```

## BeautifulSoup4
BeautifulSoup4. 줄여서 bs4로 불리기도 한다.

HTML 정보로부터 원하는 데이터를 가져오기 쉽게, 파싱할 수 있는 라이브러리이다.

파싱 방식은  html.parser, lxml, xml, html5lib 방식이 있으며, 각 특징은 다음과 같다.
![html_parser](../../imgaes/2022-02-23-bs4/html_parser.PNG)

BeautifulSoup4의 간단한 사용법은 아래와 같다

```python
import requests
from bs4 import BeautifulSoup

res = requests.get("URL")

res.raise_for_status()  

soup = BeautifulSoup(res.text, "lxml") #requests를 통해 얻은 HTML을 lxml 파서를 통해서 BeautifulSoup 객체로 만든 것
soup.a # soup 객체에서 처음 발견되는 a element를 반환
psoup.a.attrs # soup 객체에서 처음 발견되는 a element 의  속성 정보를 출력
soup.a["onclick"] # a element의 href 속성 '값' 정보를 출력
soup.find("a", attrs={"class":"Nbtn_upload"}) #class 속성의 value가 Nbtn_upload인 첫 a element를 찾아 반환
soup.find_all("a", attrs={"class":"Nbtn_upload"}) #class 속성의 value가 Nbtn_upload인 a element를 모두 찾아 List로 반환
```
2022-02-23 여기까지 일단 정리
후에 selenium과 동적 크롤링 추가 예정
## selenium